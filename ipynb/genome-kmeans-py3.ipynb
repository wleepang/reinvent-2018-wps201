{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon SageMaker with Public Datasets\n",
    "\n",
    "__*Clustering Gene Variants into Geographic Populations*__\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon SageMaker allows you to bring powerful machine learning workflows to data that is already in the cloud.  In this example, we will do just that - combining Amazon SageMaker with data from the [1000 Genomes Project] which is hosted by AWS as a [public dataset].  Specifically, we will perform unsupervised learning using Amazon SageMaker's KMeans algorithm to see if we can predict the geographic population for a set of single nucleotide polymorphisms.\n",
    "\n",
    "Single nucleotide polymorphisms or SNPs (pronounced \"snips\") are single base-pair changes to DNA.  DNA is a long chain molecule that is used to store the \"source code\" for all living organisms and is read as a sequence of four letters -- A, T, C, and G -- that are called \"bases\".  SNPs occur when a base in the sequence changes due to environmental causes or random replication errors during cell division in germ cells (eggs and sperm).  Sometimes these changes are harmless, and sometimes they can cause serious diseases.\n",
    "\n",
    "In this notebook we'll use k-means to cluster SNPs found in Chromosomes 1-22 (excluding the sex chromosomes X and Y) across the population represented in the 1000 Genomes dataset.\n",
    "\n",
    "[1000 Genomes Project]: https://aws.amazon.com/1000genomes/\n",
    "[public dataset]: https://aws.amazon.com/public-datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Data sources\n",
    "We can get variant call data (which describes SNPs, and other kinds of DNA sequence modifications) from the publicly hosted 1000 Genomes dataset on AWS.  We need the \"\\*.vcf\" files corresponding to Chromosomes 1-22 from the 20130502 release of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --human-readable s3://1000genomes/release/20130502/ | grep -P \"chr\\d+.*vcf\\.gz$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even gz compressed, these files can be over 1GB in size.  They contain a lot of information that we're not going to use in this exercise, so we need to do a little filtering.  We'll also need to some transformation of the data so that it can be fed to the Amazon Algorithm for K-Means clustering.\n",
    "\n",
    "For each chromosome's VCF we'll do the following:\n",
    "\n",
    "1. select biallelic SNPs with a frequency of 30% using a tool called [bcftools] (v1.9)\n",
    "2. select the first 100 positional SNPs and export a CSV file with a `python` script\n",
    "\n",
    "We can do the above in a scalable and parallel fashion using AWS Batch.  Each of the above steps has been containerized such that they can read and write data __*directly on S3*__.  This is ideal for cases common in genomics where the source and output data can be very large and infeasible to transfer.\n",
    "\n",
    "[bcftools]: http://samtools.github.io/bcftools/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of time, we've already done the heavy lifting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://reinvent-2018-wps201/data/ | grep -P \"chr\\d+.*freq30.*csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the curious\n",
    "if you want to give it a try on your own, the `submit_jobs.py` script found with this notebook will execute the Batch jobs needed and output the data to your own S3 bucket.  Simply run the following cell to get it started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./submit_jobs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Modeling Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Background\n",
    "\n",
    "The data comes from human genome sequences.  Humans have 23 pairs of chromosomes, and hence are \"diploid\" - meaning they should have two copies of any given DNA sequence (with a couple exceptions - e.g. genes in the XY chromosomes).\n",
    "\n",
    "A variant in a copy of a DNA sequence is called an \"allele\".  At minimum, there is at least one allele - the DNA sequence that matches the human reference genome.  Alleles that do not match the reference are called \"alternates\".\n",
    "\n",
    "A genotype is a combination of variants for a DNA sequence position, over all copies.  For example, let's say that the reference for a DNA position is 'A', and a variant for the position is 'T'.  The possible genotypes for this position would be:\n",
    "\n",
    "* A/A = REF / REF - \"homozygous\" for the reference\n",
    "* A/T = REF / ALT - \"heterozygous\"\n",
    "* T/A = ALT / REF - \"heterozygous\"\n",
    "* T/T = ALT / ALT - \"homozygous\" for the alternate\n",
    "\n",
    "Typical genotype calls use integer IDs to represent the REF and ALT alleles, with REF always being '0'.  Alternative alleles start at '1' and count up to the total number of alternative alleles for the variant.  So the possible genotypes for the example above would be:\n",
    "\n",
    "* 0 / 0\n",
    "* 0 / 1\n",
    "* 1 / 0\n",
    "* 1 / 1\n",
    "\n",
    "In cases where there is more than one alternate allele, you might see genotypes like '0 / 2' or '1 / 2'.\n",
    "\n",
    "In the CSV files above, the filtered genotypes have been converted to numerical values to make them compatible with machine learning algorithms.\n",
    "\n",
    "In this case there are three distinct values that correspond to the number of alternate allel copies a sample sequence has.\n",
    "\n",
    "* 0 / 0 = 0\n",
    "* 0 / 1 = 1\n",
    "* 1 / 0 = 1\n",
    "* 1 / 1 = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature selection\n",
    "For this modeling exercise, we are going to use a \"variant ID\" - a combination of the chromosome position, the reference allele, and the alternative allele - as labels for SNP features and the numeric genotypes described above as values for K-Means clustering.  In any given chromosome there can be 1000s of SNPs.  We want to reduce this down to a more manageable set, which can improve our clustering performance.\n",
    "\n",
    "The data processing pipeline in AWS Batch has reduced this to a more manageable set of 100 biallelic SNPs per chromosome for chromosomes 1-22, yielding 2200 features to cluster on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the panel metadata to get class labels - the geographic location that each sample originated from. Here we'll use `pandas` to process the metadata panel into classes we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pandas can read data directly from S3!\n",
    "classes = pd.read_table(\n",
    "    's3://1000genomes/release/20130502/integrated_call_samples_v3.20130502.ALL.panel', \n",
    "    usecols=['sample', 'pop', 'super_pop']\n",
    ")\n",
    "classes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data is distributed across each of the populations and super populations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "classes.groupby('pop').count().reset_index().drop(columns='super_pop').plot.bar('pop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.groupby('super_pop').count().reset_index().drop(columns='pop').plot.bar('super_pop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the charts above, there are 26 reported geographic populations and 5 super-populations in the data.  The distribution of samples across these populations looks reasonable - i.e. each group has roughly the same number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the data frame to feed into model training. To do this we'll collect the SNP counts for each chromosome from the processing pipeline into one dataframe, and merge it with the population annotations which will be used as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and aggregate SNP data from each chromosome\n",
    "import os\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "bucket_name = 'reinvent-2018-wps201'\n",
    "prefix = 'data'\n",
    "pattern = 'chr[0-9]+.*?freq30.*first100.*csv'\n",
    "\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "source_files = (\n",
    "    pd.read_csv(s3.Object(bucket_name, o.key).get()['Body'])\n",
    "    for o in bucket.objects.filter(Prefix=prefix)\n",
    "    if re.search(pattern, o.key)\n",
    ")\n",
    "\n",
    "initial_value = next(source_files).set_index('features')\n",
    "\n",
    "result = reduce(lambda x,y: pd.concat((x, y.set_index('features'))), source_files, initial_value)\n",
    "\n",
    "data = result.transpose()\n",
    "data.index.name = 'sample'\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "data = classes.merge(data, on='sample')\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "This dataset is small, only 2504 observations, but should give an idea of how to use the built-in KMeans algorithm.\n",
    "\n",
    "To prepare for training, we need to remove all non-numeric values, so below we'll drop the `pop` field from the data and store it as labels we can use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "train_data = data.copy()\n",
    "train_labels = train_data[['sample', 'pop', 'super_pop']].copy().set_index('sample')\n",
    "train_labels['pop'] = pd.Categorical(train_labels['pop'])\n",
    "train_labels['super_pop'] = pd.Categorical(train_labels['super_pop'])\n",
    "train_data = train_data.drop(columns=['pop', 'super_pop']).set_index('sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the standard setup for SageMaker training using KMeans.\n",
    "\n",
    "Be sure to set the `bucket` name to something you have access to.\n",
    "The fitting process will upload the training data to this bucket for the training instance(s) to access.  Once training is done, a model artifact will be uploaded to the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import KMeans, get_execution_role, Session\n",
    "\n",
    "session = Session()\n",
    "role = get_execution_role(sagemaker_session=session)\n",
    "bucket = '<<SageMakerS3Bucket>>'  ## per user bucket here\n",
    "\n",
    "data_location = 's3://{}/sagemaker/genome-kmeans/data'.format(bucket)\n",
    "output_location = 's3://{}/sagemaker/genome-kmeans/output'.format(bucket)\n",
    "\n",
    "print(role)\n",
    "print('training data will be uploaded to: {}'.format(data_location))\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering algorithms need to have the number of clusters specified upfront.  Some times this is difficult to know beforehand.  In this data, we know that the samples are annotated with 26 populations, and 5 super-populations, but an ideal cluster count could be anywhere between.\n",
    "\n",
    "Often, choosing a cluster count requires scanning a subset of the data and evaluating a clustering metric (i.e. mean square distance of sample to cluster center), and finding the count that is on an \"elbow\" of the resultant curve.\n",
    "\n",
    "In the cloud, we can do this efficiently with the *whole* dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = range(2, 41)\n",
    "\n",
    "# note:\n",
    "# The service limit 'Number of instances across all training jobs' is 20 Instances\n",
    "# We're queuing up 39 models in this scan, so we'll need to run in two batches\n",
    "\n",
    "models = [\n",
    "    KMeans(\n",
    "        k=k,\n",
    "        role=role,\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m4.xlarge',\n",
    "        epochs=3,\n",
    "        center_factor=4,\n",
    "        output_path=output_location,\n",
    "        data_location=data_location)\n",
    "    for k in n_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from itertools import chain, zip_longest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def chunk(iterable, chunk_size, fillvalue=None):\n",
    "    return zip_longest(*[iter(iterable)]*chunk_size, fillvalue=fillvalue)\n",
    "\n",
    "\n",
    "def get_training_job_statuses():\n",
    "    return pd.Series([\n",
    "        job['TrainingJobStatus'] \n",
    "        for job in session.sagemaker_client.list_training_jobs(MaxResults=20)['TrainingJobSummaries']\n",
    "    ])\n",
    "\n",
    "\n",
    "def all_jobs_complete():\n",
    "    return all(get_training_job_statuses() == 'Completed')\n",
    "\n",
    "\n",
    "training_jobs = []\n",
    "batch_size = 10\n",
    "for batch_num, batch in enumerate(chunk(models, batch_size)):\n",
    "    \n",
    "    for model in batch:\n",
    "        if model:\n",
    "            job_name = 'kmeans-' + datetime.now().strftime('%F-%H-%M-%S-%f')\n",
    "            model.fit(\n",
    "                model.record_set(np.float32(train_data.values)), \n",
    "                wait=False, logs=False, job_name=job_name)\n",
    "\n",
    "            training_jobs += [job_name]\n",
    "\n",
    "    print(f'Waiting for batch {batch_num}:')\n",
    "    while not all_jobs_complete():\n",
    "        print('.', end='')\n",
    "        sleep(10)\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets collect and plot the metrics from our models to see which has a good number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_k = [int(model.hyperparameters()['k']) for model in models]\n",
    "model_msd = [\n",
    "    model.training_job_analytics.dataframe().set_index('metric_name').loc['train:msd', 'value']\n",
    "    for model in models\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import plot\n",
    "\n",
    "plot(model_k, model_msd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a possible \"good\" cluster count around 5 - i.e. this is the point in the curve with the lowest metric value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Endpoint Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's deploy the models behind endpoints we can use for predictions.  This process takes about 5-9 mins for each deployment.  Here we'll create endpoints for the models with 5 and 11 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmeans_predictor_5 = models[model_k.index(5)].deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our newly deployed inference endpoint to evaluate the model.\n",
    "\n",
    "The predictor will return a results object from which we can extract the cluster assignments for each sample.  We can pass the original training data into a predictor to get cluster assignments for each sample.  To be performant, we'll pass the data to an inference endpoint in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(predictor, train_data, batch_size=200):\n",
    "    res = [\n",
    "        predictor.predict(\n",
    "            np.float32(pd.DataFrame.from_records([\n",
    "                rec for rec in record \n",
    "                if rec is not None\n",
    "            ]).drop(columns=0))\n",
    "        ) \n",
    "        for record in chunk(train_data.to_records(), batch_size)\n",
    "    ]\n",
    "\n",
    "    result = list(chain(*res))\n",
    "    clusters = np.int0([r.label['closest_cluster'].float32_tensor.values[0] for r in result])\n",
    "    \n",
    "    return clusters, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_5, result_5 = get_clusters(kmeans_predictor_5, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these predicted clusters map to the real classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    train_labels['pop'], \n",
    "    columns=clusters_5, \n",
    "    colnames=['cluster']\n",
    ").reset_index(\n",
    ").merge(\n",
    "    train_labels.reset_index()[['pop', 'super_pop']].drop_duplicates(), \n",
    "    on='pop'\n",
    ").groupby(\n",
    "    'super_pop'\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these cross tabulations we see that there are clusters with majority membership in each of our super-populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the clusters look like visually?  To answer this question, we'll generate a force weighted graph of the clusters and color code them by their original population code.\n",
    "\n",
    "To accomplish this, we'll use the [lightning-viz](http://lightning-viz.org/) package for Python.  If this notebook was created via CloudFormation, the package should already be installed in the environment.  Otherwise, you can run:\n",
    "\n",
    "```\n",
    "!pip install lightning-python\n",
    "```\n",
    "\n",
    "in a new cell before the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lightning import Lightning\n",
    "\n",
    "lgn = Lightning(ipython=True, local=True)\n",
    "\n",
    "def get_cluster_graph(result, labels):\n",
    "    graph_data = [\n",
    "        {\n",
    "            'cluster': int(r.label['closest_cluster'].float32_tensor.values[0]),\n",
    "            'distance': float(r.label['distance_to_cluster'].float32_tensor.values[0])\n",
    "        }\n",
    "        for r in result\n",
    "    ]\n",
    "\n",
    "    gg = pd.concat(\n",
    "        (labels.reset_index(), \n",
    "         pd.DataFrame(graph_data)),\n",
    "        axis=1\n",
    "    )\n",
    "    gg['code'] = pd.np.NaN  # place holder for population category codes\n",
    "    \n",
    "    centers = sorted(gg['cluster'].unique())\n",
    "\n",
    "    gg = pd.concat(\n",
    "        (pd.DataFrame({  # this dataframe is for the cluster centers\n",
    "            'cluster': centers, \n",
    "            'distance': 0, \n",
    "            'sample': list(map(str, centers)), \n",
    "            'pop': '',\n",
    "            'super_pop': ''\n",
    "        }), gg)).reset_index().drop(columns='index')\n",
    "    gg['code'] = pd.Categorical(gg['super_pop']).codes\n",
    "\n",
    "\n",
    "    # generate the network links and plot\n",
    "    nn = [(r[0], r[1], r[2]) for r in gg.to_records()]\n",
    "    \n",
    "    return nn, gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data with 5 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn, gg = get_cluster_graph(result_5, train_labels)\n",
    "lgn.force(nn, group=gg['code'], labels=gg['sample'] + ' ' + gg['pop'] + ':' + gg['super_pop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom Line\n",
    "\n",
    "The African (AFR) and East Asian (EAS) super-populations produce good clusters.\n",
    "\n",
    "The mixture of populations in the clusters may be interpretted as individuals with mixed ancestry or possibly immigration.  Also, the clustering could be improved further if there was additional dimensionality reduction (e.g. via PCA), more samples, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete the Endpoint(s)\n",
    "If you're ready to be done with this notebook, make sure run the cell below.  This will remove any hosted endpoints you created and avoid any charges from unused resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_predictor_5.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
